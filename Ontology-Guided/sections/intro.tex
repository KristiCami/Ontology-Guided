\section{INTRODUCTION}
Modern software systems are defined through sets of func\-tional and non--functional requirements that are usually written in natural language (NL) by stakeholders, engineers, or domain experts. Although natural language is flexible and widely understood, it is also inherently ambiguous, incomplete, and inconsistent. Such issues pose serious obstacles to automated validation, requirement tracing, and reuse in downstream software engineering tasks. In practice, many system failures have been linked to poorly specified or misinterpreted requirements, underscoring the need for formal and machine--readable rep\-resentations.

One promising direction is to translate requirements into ontologies expressed in the Web Ontology Language (OWL). Ontologies enable reasoning over domain concepts, properties, and constraints, thus supporting tasks such as consistency checking, compliance verification, and knowledge reuse. How\-ever, manual ontology construction from textual requirements is costly, error--prone, and does not scale. Traditional ontology learning approaches, while partially automating extraction, often produce flat term lists or isolated entities that lack logical rigor and require extensive human curation.

Recent advances in Large Language Models (LLMs), such as GPT--4, provide new opportunities for semantic interpre\-tation of requirements. LLMs can capture linguistic nuance and propose candidate OWL axioms directly from raw text. Yet, their outputs are not guaranteed to be logically consistent or semantically valid. Without symbolic safeguards, generated ontologies may contain contradictions, missing relations, or domain misalignments.

To address these challenges, we propose a neuro--symbolic pipeline for ontology--guided ontology drafting from re\-quirements. Our approach integrates neural generation with symbolic validation and repair in a closed loop. Specifically, requirement sentences are first interpreted by an LLM into candidate OWL axioms. These axioms are aligned with pre\-defined ontologies, validated against SHACL constraints, and checked by logical reasoners. Any violations are transformed into structured repair prompts that guide the LLM to iteratively revise its output until a conformant ontology is obtained.

This work makes the following contributions:
\begin{itemize}
\item Ontology--aware LLM prompting: We enhance neural generation by injecting available domain vocabularies into prompts, reducing semantic drift and improving alignment with existing ontologies.
\item Constraint--guided iterative repair: We introduce a novel feedback mechanism where SHACL and reasoner violations are automatically converted into targeted repair prompts, enabling self--correcting OWL generation.
\item Hybrid neuro--symbolic integration: We demonstrate that the combination of LLMs with OWL reasoning and SHACL validation yields more precise and consistent ontologies than neural--only or symbolic--only baselines.
\item Domain adaptability: We show that the same pipeline generalizes across domains (ATM, healthcare, automo\-tive) by swapping ontology vocabularies and shape con\-straints without retraining the neural model.
\end{itemize}

In summary, this paper presents a novel closed--loop neuro--symbolic framework that transforms informal natural language requirements into validated ontologies. The resulting OWL ontologies can be applied to consistency checking, requirement tracing, and, in the future, automated code synthesis.
