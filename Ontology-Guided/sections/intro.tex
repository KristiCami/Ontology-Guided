\section{INTRODUCTION}
Modern software systems are defined through sets of functional and non--functional requirements that are usually written in natural language (NL) by stakeholders, engineers, or domain experts. Although natural language is flexible and widely understood, it is also inherently ambiguous, incomplete, and inconsistent. Such issues pose serious obstacles to automated validation, requirement tracing, and reuse in downstream software engineering tasks. In practice, many system failures have been linked to poorly specified or misinterpreted requirements, underscoring the need for formal and machine--readable representations.

One promising direction is to translate requirements into ontologies expressed in the Web Ontology Language (OWL). Ontologies enable reasoning over domain concepts, properties, and constraints, thus supporting tasks such as consistency checking, compliance verification, and knowledge reuse. However, manual ontology construction from textual requirements is costly, error--prone, and does not scale. Traditional ontology learning approaches, while partially automating extraction, often produce flat term lists or isolated entities that lack logical rigor and require extensive human curation.

Recent advances in Large Language Models (LLMs), such as GPT--4, provide new opportunities for semantic interpretation of requirements. LLMs can capture linguistic nuance and propose candidate OWL axioms directly from raw text. Yet, their outputs are not guaranteed to be logically consistent or semantically valid. Without symbolic safeguards, generated ontologies may contain contradictions, missing relations, or domain misalignments.

To address these challenges, we propose a neuro--symbolic pipeline for ontology--guided drafting of OWL ontologies from requirements. Our approach integrates neural generation with symbolic validation and repair in a closed loop. Specifically, requirement sentences are first interpreted by an LLM into candidate OWL axioms. These axioms are aligned with predefined ontologies, validated against SHACL constraints, and checked by logical reasoners. Any violations are transformed into structured repair prompts that guide the LLM to iteratively revise its output until a conformant ontology is obtained.

This work makes the following contributions:
\begin{itemize}
\item \textbf{Ontology--aware LLM prompting:} Neural generation is enhanced by injecting available domain vocabularies into prompts, reducing semantic drift and improving alignment with existing ontologies.
\item \textbf{Constraint--guided iterative repair:} A novel feedback mechanism is introduced where SHACL and reasoner violations are automatically converted into targeted repair prompts, enabling self--correcting OWL generation.
\item \textbf{Hybrid neuro--symbolic integration:} The combination of LLMs with OWL reasoning and SHACL validation yields more precise and consistent ontologies than neural--only or symbolic--only baselines.
\item \textbf{Domain adaptability:} The same pipeline generalizes across domains (ATM, healthcare, automotive) by swapping ontology vocabularies and shape constraints without retraining the neural model.
\end{itemize}

In summary, this paper presents a closed--loop neuro--symbolic framework that transforms informal natural language requirements into validated OWL ontologies. The resulting ontologies can be applied to consistency checking, requirement tracing, and, in future work, automated code synthesis.
