# 5.1.2 Config & Execution

Η εκτέλεση του OG–NSD οργανώνεται γύρω από ένα σαφές σχήμα παραμετροποίησης (configuration) που καθιστά τη διαδικασία πλήρως αναπαραγώγιμη. Ο πυρήνας της διαμόρφωσης είναι η `PipelineConfig` στο `og_nsd/config.py`, η οποία ορίζει όλα τα κρίσιμα πεδία: paths για requirements/shapes/base/CQs, αριθμό iterations του repair loop, επιλογή LLM mode (`openai` ή `heuristic`), όρια sampling (π.χ. `max_reqs`) και flags όπως `draft_only` ή `reasoning`. Έτσι, ο χρήστης δεν χρειάζεται να τροποποιεί κώδικα για να τρέξει παραλλαγές του pipeline· αρκεί η εναλλαγή ενός αρχείου JSON ή η αλλαγή CLI παραμέτρων.

Η εκτέλεση στο CLI γίνεται μέσω του `scripts/run_pipeline.py`, το οποίο λειτουργεί ως “adapter” ανάμεσα στις επιλογές χρήστη και στην κλάση `OntologyDraftingPipeline` του `og_nsd/pipeline.py`. Το script μεταφράζει τα CLI arguments σε αντικείμενο config και ξεκινά την βασική ροή: φόρτωση απαιτήσεων, δημιουργία draft, επικύρωση, ενδεχόμενη επαναληπτική επισκευή. Η ύπαρξη του script επιτρέπει και τις δύο μορφές εκτέλεσης: ad-hoc runs (π.χ. με `--max-reqs 50`) αλλά και πλήρως προσδιορισμένες εκτελέσεις μέσω config αρχείων.

Για τα κύρια πειράματα (E1–E6), υπάρχουν ειδικά scripts όπως `scripts/run_e1_llm_only.py`, `scripts/run_e2_symbolic_only.py` και `scripts/run_e4_iterative.py`. Αυτά χαρτογραφούν 1-προς-1 τις πειραματικές ρυθμίσεις και χρησιμοποιούν αντίστοιχα config files στον φάκελο `configs/`. Για παράδειγμα, το `configs/atm_e4_iterative.json` καθορίζει ότι η εκτέλεση είναι πλήρης (draft + validation + repair loop), ενώ το `configs/atm_e1_llm_only.json` απενεργοποιεί τον SHACL και reasoning έλεγχο. Το αποτέλεσμα είναι ένα σαφές πρωτόκολλο εκτέλεσης όπου κάθε experiment έχει δικό του script και δικό του configuration snapshot.

Σημαντικό μέρος του execution pipeline είναι ο έλεγχος LLM mode. Αν υπάρχει διαθέσιμο `OPENAI_API_KEY`, το `og_nsd/llm.py` θα χρησιμοποιήσει OpenAI backend· εναλλακτικά, το heuristic mode παρέχει deterministic rule-based drafting, κρίσιμο για offline αναπαραγωγιμότητα. Ο χρήστης ελέγχει πλήρως αυτή τη συμπεριφορά μέσω `--llm-mode`, ενώ στο ontology-aware prompting μπορούν να περαστούν επιπλέον flags (`--use-ontology-context`, `--ontology-context`).

Τέλος, η εκτέλεση καταγράφει αποτελέσματα σε συγκεκριμένο output path, είτε μέσω CLI `--output/--report` είτε μέσω των defaults του config. Τα scripts γράφουν σε `build/` ή `runs/` ανάλογα με το πείραμα. Αυτή η προτυποποιημένη εκτέλεση είναι κρίσιμη για ακαδημαϊκές εργασίες, επειδή επιτρέπει να επαναληφθεί ακριβώς η ίδια διαδικασία και να παραχθούν τα ίδια artefacts, κάτι που είναι βασική απαίτηση επιστημονικής τεκμηρίωσης.
