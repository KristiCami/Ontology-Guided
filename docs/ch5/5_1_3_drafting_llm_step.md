# 5.1.3 Drafting / LLM Step

Το στάδιο drafting αποτελεί τον “νευρωνικό” πυρήνα του OG–NSD. Η είσοδος είναι καθαρές απαιτήσεις φυσικής γλώσσας και η έξοδος ένα αρχικό Turtle γράφημα με OWL axioms. Η υλοποίηση βρίσκεται στο `og_nsd/llm.py`, όπου ο adapter διαχειρίζεται δύο modes: (α) OpenAI backend, που κάνει πραγματική κλήση σε LLM, και (β) heuristic fallback, που αναπαράγει κανόνες παραγωγής ώστε να υπάρχει offline αναπαραγωγιμότητα. Ο pipeline πυρήνας (`og_nsd/pipeline.py`) καλεί το LLM component και καταγράφει τις παραγόμενες αξιωματικές δηλώσεις ως draft ontology.

Η φάση αυτή ενσωματώνει ontology-aware prompting όταν ο χρήστης το ενεργοποιεί. Με βάση το README, το σύστημα εξάγει λεξιλόγιο από ένα υπάρχον TTL (π.χ. `gold/atm_gold.ttl`) και κατασκευάζει structured context (classes, object/datatype properties, domain/range, prefixes). Το context εισάγεται στο prompt ως “συμβόλαιο” και όχι ως αντιγραφή οντολογίας. Αυτό μειώνει lexical drift, επιβάλλει σωστό namespace και οδηγεί το LLM να χρησιμοποιήσει ήδη υπάρχουσες οντολογικές έννοιες αντί να invent names. Η σχεδίαση αυτή είναι κρίσιμη για ακαδημαϊκή αξιολόγηση, καθώς διαχωρίζει σαφώς τη “γενετική” πληροφορία (requirements) από το “καθοδηγητικό λεξιλόγιο”.

Στο draft-only baseline (E1), η διαδικασία σταματά μετά το πρώτο LLM output (`--draft-only`). Έτσι, το παραγόμενο Turtle (`pred.ttl`) αντιστοιχεί σε μη επικυρωμένη οντολογία, η οποία χρησιμοποιείται ως baseline για σύγκριση. Αντίθετα, στο “ontology-aware” draft (E3) ο LLM μπαίνει σε constrained mode και παράγει draft που τείνει να είναι πιο συνεπές με το gold schema. Στην πράξη, το ίδιο input απαιτήσεων οδηγεί σε διαφορετική ποιότητα axioms, επιτρέποντας ποσοτική αξιολόγηση των επιδράσεων του prompting.

Στην υλοποίηση, το drafting step είναι ενσωματωμένο στην αρχή της pipeline ροής και ακολουθείται από serialization σε Turtle μέσω του `og_nsd/ontology.py`. Η έξοδος καταγράφεται ως προσωρινό γράφημα και χρησιμοποιείται ως αρχική κατάσταση για την επόμενη φάση επικύρωσης. Σημαντικό είναι ότι το LLM δεν “διορθώνει” απευθείας το γράφημα· παράγει νέο κείμενο Turtle που γίνεται parse/merge. Αυτό επιτρέπει καθαρό logging και αναπαραγωγή: κάθε draft αποθηκεύεται ως artefact στο `runs/` ή `build/`, διατηρώντας ίχνη του αρχικού LLM output.

Συνολικά, το drafting step λειτουργεί ως γεννήτρια γνώσης, ενώ η υπόλοιπη pipeline επιβάλλει περιορισμούς, διορθώσεις και επικύρωση. Η αυστηρή οριοθέτηση ανάμεσα σε LLM output και symbolic έλεγχο είναι καθοριστική για την ακαδημαϊκή τεκμηρίωση της υλοποίησης, αφού επιτρέπει να μελετηθεί ανεξάρτητα η συμβολή του LLM και της επακόλουθης repair διαδικασίας.
